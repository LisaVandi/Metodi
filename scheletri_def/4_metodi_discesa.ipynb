{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as npl\n",
    "import SolveTriangular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tratta di metodi iterativi per la risoluzione di sistemi lineari Ax = b, \n",
    "con A matrice simmetrica e definita positiva (quindi quadrata). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si applica il teorema che afferma: \n",
    "    \"Data A matrice quadrata, simmetrica e definita positiva, allora la soluzione del sistema lineare Ax = b \n",
    "    coincide con il punto di minimo della funzione quadratica F(x) = 1/2 * <Ax, x> - <b, x>\". \n",
    "    Tale teorema si basa su: \n",
    "        1. definizione del vettore residuo r = Ax - b\n",
    "        2. calcolo del gradiente di f(x) per trovare il minimo e porlo = 0\n",
    "        3. verifica del punto di minimo attraverso la matrice Hessiana: coincide con A, quindi x.T * A * x > 0,\n",
    "        cioè F convessa e ha un solo punto di minimo (quello trovato)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oltre al vettore residuo, si trova anche il vettore direzione p, in maniera tale che l'iterato successivo diminuisca, \n",
    "in funzione dello step size alfa, il cui valore si trova minimizzando F(xk + alfa* pk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo step-size si ottiene così: alfa = -(r.T @ p) / (p.T * A@p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Metodo steepest descent\n",
    "\n",
    "Si sceglie il vettore direzione come antigradiente (direzione di massima decrescita), cioè pk = -rk\n",
    "x(k+1) = x(k) + alpha*direzione\n",
    "\n",
    "Si nota che il metodo dell'antigradiente ha un andamento a zig zag, dovuto al fatto che il gradiente di un iterato è ortogonale\n",
    "al gradiente dell'iterato precedente, quindi la velocità di convergenza è bassa. \n",
    "\"\"\"\n",
    "def steepestdescent(A,b,x0,itmax,tol):\n",
    "    n,m=A.shape\n",
    "    if n!=m:\n",
    "        print(\"Matrice non quadrata\")\n",
    "        return [],[]\n",
    "\n",
    "   # inizializzare le variabili necessarie\n",
    "    x = x0   \n",
    "    r = A@x-b\n",
    "    p = -r # to do\n",
    "    it = 0\n",
    "    nb=np.linalg.norm(b)\n",
    "    errore=np.linalg.norm(r)/nb\n",
    "    vec_sol=[]\n",
    "    vec_sol.append(x)\n",
    "    vet_r=[]\n",
    "    vet_r.append(errore)\n",
    "     \n",
    "# utilizzare il metodo del gradiente per trovare la soluzione\n",
    "    while it<=itmax and errore >= tol: #to do:\n",
    "        it=it+1\n",
    "        Ap = A@p #to do       \n",
    "        alpha = -(r.T @ p) / (p.T * Ap) #to do\n",
    "        x = x + alpha * p # to do\n",
    "\n",
    "        vec_sol.append(x)\n",
    "        r=r+alpha*Ap\n",
    "        errore=np.linalg.norm(r)/nb\n",
    "        vet_r.append(errore)\n",
    "        p = - r #to do\n",
    "        \n",
    "    return x,vet_r,vec_sol,it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Metodo del gradiente coniugato\n",
    "\n",
    "Si ha: p = -r + gamma * p_prec \n",
    "       gamma = <r, Ap> / <p, Ap>\n",
    "       x = x_prec + alfa * p\n",
    "\"\"\"\n",
    "def conjugate_gradient(A,b,x0,itmax,tol):\n",
    "    n,m=A.shape\n",
    "    if n!=m:\n",
    "        print(\"Matrice non quadrata\")\n",
    "        return [],[]\n",
    "    \n",
    "   # inizializzare le variabili necessarie\n",
    "    x = x0     \n",
    "    r = A@x-b\n",
    "    p = -r\n",
    "    it = 0\n",
    "    nb=np.linalg.norm(b)\n",
    "    errore=np.linalg.norm(r)/nb\n",
    "    vec_sol=[]\n",
    "    vec_sol.append(x0)\n",
    "    vet_r=[]\n",
    "    vet_r.append(errore)\n",
    "# utilizzare il metodo del gradiente coniugato per calcolare la soluzione\n",
    "    while errore >= tol and it< itmax:\n",
    "        it=it+1\n",
    "        Ap = A@p #to do\n",
    "        alpha = (r.T @ r) / (p.T @ Ap) #to do\n",
    "        x = x + alpha * p #to do\n",
    "        vec_sol.append(x)\n",
    "        rtr_old=r.T@r\n",
    "        r=r+alpha*Ap\n",
    "        gamma = (r.T@r)/rtr_old # to do\n",
    "        errore=np.linalg.norm(r)/nb\n",
    "        vet_r.append(errore)\n",
    "        p = -r + gamma * p #to do\n",
    "   \n",
    "    return x,vet_r,vec_sol,it\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
