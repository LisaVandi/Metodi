{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77e97ec2-857d-44c0-8b7f-c9b2c436362e",
   "metadata": {},
   "source": [
    "## Esame di Metodi Numerici 6 Maggio 2024 \n",
    "\n",
    "## Esercizio 1\n",
    "- Si consideri il sistema lineare Ax=b, con A matrice e b termine noto memorizzati nel file ``'test_14_09_2023.mat'``.  Risolvere il sistema confrontando almeno due tra i metodi  visti  per utilizzare per risolvere il sistema lineare con tale matrice dei coefficienti. Confrontare i risultati dei vari metodi, e giustificare i loro comportamento utilizzando i risultati teorici visti a lezione.\n",
    "\n",
    "Per la lettura dei dati procedere nel seguente modo:\n",
    "\n",
    "``from scipy.io import loadmat``\n",
    "\n",
    "``import numpy as np``\n",
    "\n",
    "``dati = loadmat('test_06_05_2024.mat')``\n",
    "\n",
    "``A=dati[\"A\"] ``\n",
    "\n",
    "``A=A.astype(float)``\n",
    "\n",
    "`` b=dati[\"b\"] ``\n",
    "\n",
    "`` b=b.astype(float)``\n",
    "\n",
    "\n",
    "                                       [10 punti]\n",
    "                                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0299e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import numpy.linalg as npl\n",
    "import matplotlib.pyplot as plt\n",
    "import SolveTriangular\n",
    "import scipy.linalg as spl\n",
    "\n",
    "dati = loadmat('test_06_05_2024.mat')\n",
    "A=dati[\"A\"]\n",
    "A=A.astype(float)\n",
    "b=dati[\"b\"] \n",
    "b=b.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03be5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensioni matrice A = 400 x 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14a702dc810>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scelta del metodo in base alla matrice\n",
    "m, n = A.shape\n",
    "print(f'dimensioni matrice A = {m} x {n}')\n",
    "plt.spy(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b69c5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrice A quadrata e di grande dimensioni\n",
      "matrice A sparsa\n"
     ]
    }
   ],
   "source": [
    "print(\"matrice A quadrata e di grande dimensioni\")\n",
    "# controllo se è sparsa o densa\n",
    "if (np.count_nonzero(A) * 100 / (m*n)) < 33:\n",
    "    print(\"matrice A sparsa\")\n",
    "else: print(\"matrice A densa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2881f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrice A simmetrica\n",
      "matrice A definita positiva\n"
     ]
    }
   ],
   "source": [
    "if np.all(A == A.T): \n",
    "    print(\"matrice A simmetrica\")\n",
    "    if np.all(npl.eigvals(A) > 0): \n",
    "        print(\"matrice A definita positiva\")\n",
    "    else: print(\"matrice A non definita positiva\")\n",
    "else: print(\"matrice A non simmetrica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb5b52",
   "metadata": {},
   "source": [
    "Poiché la matrice è quadrata, di grandi dimensioni e sparsa, scelgo tra i metodi iterativi. In particolare, essendo simmetrica e definita positiva, implemento il metodo steepest descent e conjugate gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d653f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepestdescent(A,b,x0,itmax,tol):\n",
    "    n,m=A.shape\n",
    "    if n!=m:\n",
    "        print(\"Matrice non quadrata\")\n",
    "        return [],[]\n",
    "    \n",
    "   # inizializzare le variabili necessarie\n",
    "    x = x0   \n",
    "    r = A@x-b\n",
    "    p = -r \n",
    "    it = 0\n",
    "    nb=np.linalg.norm(b)\n",
    "    errore=np.linalg.norm(r)/nb\n",
    "    vec_sol=[]\n",
    "    vec_sol.append(x)\n",
    "    vet_r=[]\n",
    "    vet_r.append(errore)\n",
    "     \n",
    "# utilizzare il metodo del gradiente per trovare la soluzione\n",
    "    while errore >= tol and it < itmax: \n",
    "        it=it+1\n",
    "        Ap = A @ p       \n",
    "        alpha = -(r.T @ p) / (p.T @ Ap)                 \n",
    "        x = x + alpha * p         \n",
    "        vec_sol.append(x)\n",
    "        r=r+alpha*Ap\n",
    "        errore=np.linalg.norm(r)/nb\n",
    "        vet_r.append(errore)\n",
    "        p = -r\n",
    "     \n",
    "    return x,vet_r,vec_sol,it\n",
    "\n",
    "\n",
    "def conjugate_gradient(A,b,x0,itmax,tol):\n",
    "    n,m=A.shape\n",
    "    if n!=m:\n",
    "        print(\"Matrice non quadrata\")\n",
    "        return [],[]\n",
    "    \n",
    "   # inizializzare le variabili necessarie\n",
    "    x = x0     \n",
    "    r = A@x-b\n",
    "    p = -r\n",
    "    it = 0\n",
    "    nb=np.linalg.norm(b)\n",
    "    errore=np.linalg.norm(r)/nb\n",
    "    vec_sol=[]\n",
    "    vec_sol.append(x0)\n",
    "    vet_r=[]\n",
    "    vet_r.append(errore)\n",
    "# utilizzare il metodo del gradiente coniugato per calcolare la soluzione\n",
    "    while errore >= tol and it< itmax:\n",
    "        it=it+1\n",
    "        Ap = A@p\n",
    "        alpha = -(r.T @ p) / (p.T @ Ap)\n",
    "        x = x + alpha * p\n",
    "        vec_sol.append(x)\n",
    "        rtr_old=r.T@r\n",
    "        r=r+alpha*Ap\n",
    "        gamma = (r.T @ r) / rtr_old\n",
    "        errore=np.linalg.norm(r)/nb\n",
    "        vet_r.append(errore)\n",
    "        p = r + gamma * p\n",
    "   \n",
    "    \n",
    "    return x,vet_r,vec_sol,it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2aa92a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soluzione con steepest_descent x = [[0.99999981 0.99999981 0.99999981 ... 0.99999981 0.99999981 0.99999981]\n",
      " [1.00000012 1.00000012 1.00000012 ... 1.00000012 1.00000012 1.00000012]\n",
      " [1.00000012 1.00000012 1.00000012 ... 1.00000012 1.00000012 1.00000012]\n",
      " ...\n",
      " [1.00000012 1.00000012 1.00000012 ... 1.00000012 1.00000012 1.00000012]\n",
      " [1.00000012 1.00000012 1.00000012 ... 1.00000012 1.00000012 1.00000012]\n",
      " [0.99999981 0.99999981 0.99999981 ... 0.99999981 0.99999981 0.99999981]] in 62 iterazioni\n"
     ]
    }
   ],
   "source": [
    "tol = 1e-6\n",
    "itmax = 400\n",
    "x0 = np.array([0] * n).T\n",
    "x_st, vet_r_st, vec_sol_st, it_st = steepestdescent(A,b,x0,itmax,tol)\n",
    "print(f'Soluzione con steepest_descent x = {x_st} in {it_st} iterazioni')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a18719b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soluzione con conjugate_gradient x = [[0.99999998 0.99999998 0.99999998 ... 0.99999998 0.99999998 0.99999998]\n",
      " [1.00000001 1.00000001 1.00000001 ... 1.00000001 1.00000001 1.00000001]\n",
      " [1.00000001 1.00000001 1.00000001 ... 1.00000001 1.00000001 1.00000001]\n",
      " ...\n",
      " [1.00000001 1.00000001 1.00000001 ... 1.00000001 1.00000001 1.00000001]\n",
      " [1.00000001 1.00000001 1.00000001 ... 1.00000001 1.00000001 1.00000001]\n",
      " [0.99999998 0.99999998 0.99999998 ... 0.99999998 0.99999998 0.99999998]] in 24 iterazioni\n"
     ]
    }
   ],
   "source": [
    "x_conj, vet_r_conj, vec_sol_conj, it_conj = conjugate_gradient(A,b,x0,itmax,tol)\n",
    "print(f'Soluzione con conjugate_gradient x = {x_conj} in {it_conj} iterazioni')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5031f5",
   "metadata": {},
   "source": [
    "Si nota che il metodo del gradiente coniugato ottiene la medesima soluzione con un numero di iterazioni molto più basso. Infatti, nel metodo del gradiente coniugato, il vettore direzione è variato di una certa quantità gamma, in maniera tale che le direzioni di discesa siano coniugate rispetto a tutte le precedenti direzioni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0d44a-0d01-44c1-b47f-9a155608f40b",
   "metadata": {},
   "source": [
    "- Data la matrice \n",
    "$$A=\\left[\n",
    "\\begin{array}{cccc}\n",
    "1 & 2 & 3 & 4\\\\\n",
    "2 & -4 & 6 & 8\\\\\n",
    "-1 & -2 & -3 & -1\\\\\n",
    "5 & 7 & 0 & 1\n",
    "\\end{array}\n",
    "\\right ],$$\n",
    "Richiamare le ipotesi sotto cui esiste la fattorizzazione di Gauss senza pivoting e scrivere un codice per  verificarle.\n",
    "\n",
    "                                                [2 punti]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6c062",
   "metadata": {},
   "source": [
    "Per applicare Gauss senza pivotaggio (Gauss semplice), è necessario che: \n",
    "- la matrice sia ben condizionata\n",
    "- la matrice sia non singolare, cioè abbia determinante != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "212cf5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gauss(A, b): \n",
    "    # Passo 1: Fattorizzazione LU di A\n",
    "    P, L, U = spl.lu(A) \n",
    "\n",
    "    # Passo 2: Risoluzione del sistema Ly = Pb\n",
    "    y,flag=SolveTriangular.Lsolve(L,P@b) \n",
    "    if flag==0: # soluzione trovata con successo\n",
    "        # Passo 3: Risoluzione del sistema Ux = y\n",
    "        x,flag=SolveTriangular.Usolve(U,y) \n",
    "    else:\n",
    "        return None,None\n",
    "    return x,flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30beddff",
   "metadata": {},
   "source": [
    "## Esercizio 2\n",
    "Scrivere uno script che calcoli il polinomio interpolante un insieme di punti $P_i =(x_i, y_i)$ $i = 0, ..., n $ nella forma di Lagrange, $n=5,10,15,18$\n",
    "\n",
    "- nodi $x_i$, punti equidistanti in un intervallo $[a, b]$,\n",
    "- nodi $x_i$, zeri dei polinomi di Chebyshev nell'intervallo $[a, b]$, ossia\n",
    "$$\n",
    "x_i = \\frac{(a + b)}{2}+\\frac{(b-a)}{2} \\, \\cos \\left(\n",
    "\\frac{(2i+1)\\pi}{2(n + 1)}\n",
    "\\right), \\quad  i =0, ..., n \n",
    "$$\n",
    " \n",
    "  e $y_i = f(x_i)$ ottenuti dalla valutazione nei punti $x_i$ della funzione test   $f: \\ [a, b] \\rightarrow {\\mathbb R}$. \n",
    "  - $f(x) = 1/(1+25*x^2)$,  $ \\quad x \\in [-1, 1]$ (funzione di Runge).\n",
    "  \n",
    "                                          [6] punti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a990016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1e4a7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plagr(xnodi,j):\n",
    "    \"\"\"\n",
    "    Restituisce i coefficienti del j-esimo pol di\n",
    "    Lagrange associato ai punti del vettore xnodi\n",
    "    \"\"\"\n",
    "    xzeri=np.zeros_like(xnodi)\n",
    "    n=xnodi.size\n",
    "    if j==0:\n",
    "       xzeri=xnodi[1:n]\n",
    "    else:\n",
    "       xzeri=np.append(xzeri[0:j], xzeri[j+1, n])\n",
    "\n",
    "    num = np.poly(xzeri) \n",
    "    den = np.polyval(num, xnodi[j])\n",
    "    p=num/den\n",
    "    \n",
    "    return p\n",
    "\n",
    "def InterpL(x, y, xx):\n",
    "     \"\"\"\n",
    "     funzione che determina in un insieme di punti il valore del polinomio\n",
    "     interpolante ottenuto dalla formula di Lagrange.\n",
    "     DATI INPUT\n",
    "        x  vettore con i nodi dell'interpolazione\n",
    "        f  vettore con i valori dei nodi \n",
    "        xx vettore con i punti in cui si vuole calcolare il polinomio\n",
    "     DATI OUTPUT\n",
    "        y vettore contenente i valori assunti dal polinomio interpolante\n",
    "        \"\"\"\n",
    "     n = x.size\n",
    "     m = xx.size\n",
    "     L=np.zeros((m,n))\n",
    "     for j in range(n):\n",
    "        p=plagr(L, j)\n",
    "        L[:,j]= np.polyval(p, xx)   \n",
    "    \n",
    "     return L@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5823efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeri_chebichev(a, b, n):\n",
    "    membro_1 = (a + b) / 2\n",
    "    membro_2 = (b - a) / 2\n",
    "    x = np.zeros((n+1,))\n",
    "\n",
    "    for i in range(n):\n",
    "        x[i] = membro_1 + membro_2 * np.cos(((2 * i + 1) * math.pi) / (2 * (n + 1)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a8eba0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "a = -1\n",
    "b = 1\n",
    "\n",
    "x_eq = np.linspace(a, b, n+1)\n",
    "x_cheb = zeri_chebichev(a, b, n)\n",
    "xx = np.linspace(a, b, 100)\n",
    "\n",
    "f = lambda x: 1/(1+25*x**2)\n",
    "y_eq = f(x_eq)\n",
    "y_cheb = f(x_cheb)\n",
    "y_fun = f(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbda924-2d3b-4357-bf34-cb936d3a4a87",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "- Calcolare l'errore di interpolazione $r(x) =  f(x)-pe(x) $,\n",
    "tra la funzione test $f(x)$ e il polinomio di interpolazione $pe(x)$ calcolato a partire da nodi equdisitanti.\n",
    "                                        [1] punto\n",
    "                                        \n",
    "Visualizzare il grafico di $f(x)$ e $pe(x)$, ed il grafico di $|r(x)|$ per ogni valore $n=5,10,15,18$ \n",
    "\n",
    "                                        [1] punto\n",
    "                                        \n",
    "Calcolare l'errore di interpolazione $r(x) =  f(x)-pc(x) $,\n",
    "tra la funzione test $f(x)$ e il polinomio di interpolazione $p(x)$ calcolato a partire da nodi di Chebichev.\n",
    "\n",
    "                                      [1] punto\n",
    "                                            \n",
    "Visualizzare il grafico di $f(x)$ e $pc(x)$, ed il grafico di $|r(x)|$. \n",
    "\n",
    "                                       [1] punto\n",
    "\n",
    "Cosa si osserva? Cosa accade all'aumentare del grado $n$ di $p(x)$? Scrivere la formula dell'errore che si compie quando al posto della funzione che ha generato i dati si considera il polinomio interpolatore di grado n e commentarla.\n",
    "                                         \n",
    "                                         [3 punti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4802e319",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InterpL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mCalcolare l'errore di interpolazione r(x) =  f(x)-pe(x),\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mtra la funzione test $f(x)$ e il polinomio di interpolazione pe(x) calcolato a partire da nodi equidistanti.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m pe \u001b[38;5;241m=\u001b[39m InterpL(x_eq, y_eq, xx)\n\u001b[0;32m      6\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: f(x) \u001b[38;5;241m-\u001b[39m pe(x)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'InterpL' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Calcolare l'errore di interpolazione r(x) =  f(x)-pe(x),\n",
    "tra la funzione test $f(x)$ e il polinomio di interpolazione pe(x) calcolato a partire da nodi equidistanti.\n",
    "\"\"\"\n",
    "pe = InterpL(x_eq, y_eq, xx)\n",
    "r = lambda x: f(x) - pe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4239d5c-a24b-432f-9903-e5d5e67f0669",
   "metadata": {},
   "source": [
    "**Domanda AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8871d473",
   "metadata": {},
   "source": [
    "1. Descrivere gli elementi caratterizzanti di un MultiLayer Perceptron (MLP). (Com'è fatto un neurone artificiale, a caso servono le funzioni di attivazione, come sono organizzati i neuroni. Varie tipologie di reti MLP)  ed accennare in cosa consiste la fase di forward propagation e la fase di backward propagation. **Punti: 1**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un neurone artificiale, detto percettrone, riceve degli input, i0, i1, ..., in, ai quali sono associati dei pesi (w1, w2, ..., wn), tra cui in particolare si ricorda w0, il bias, cioè un peso collegato a un input fittizio con valore sempre pari a 1, che viene utilizzato per tarare il punto di lavoro ottimale del neurone. \n",
    "La funzione di attivazione f determina il comportamento del neurone, cioè il suo output z. Essa simula il comportamento del neurone biologico di attivarsi solo se i segnali in ingresso superano una certa soglia. \n",
    "Un MLP (multilayer perceptron) è costituito da più layer di neuroni: tipicamente si ha 1 input layer, 1 o più hidden layer, e 1 output layer.\n",
    "\n",
    "Nella fase di forward propagation, i dati in input vengono propagati strato per strato attraverso la rete. Per ogni neurone si calcola la funzione di attivazione.\n",
    "Nella fase di backward propagation, si calcolano i gradienti della funzione di perdita rispetto a ciascun peso della rete, utilizzando la chain rule per il calcolo differenziale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec61b1b",
   "metadata": {},
   "source": [
    "2. Ottimizzazione della loss function per il training di una rete neurale per il task di regressione: Metodo di discesa del gradiente, metodo stocastico del gradiente, metodo del gradiente minibatch.  **Punti 1**  \n",
    " - Non convessità della loss-function - come non rimanere bloccati in un monimo locale? Metodo del gradiente con momentum. **Punti 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b25b6",
   "metadata": {},
   "source": [
    "Al fine di non rimanere bloccati in un punto di minimo locale, è necessario che la loss function sia convessa. Infatti, questo assicura che il punto di minimo trovato con il metodo del gradiente sia globale e non locale. La funzione costo priva di hidden layer è convessa. Qualora si aggiungano hidden layer, si incorre in una maggiore facilità di imbattersi in punti di sella o minimi locali.\n",
    "\n",
    "\n",
    "Il metodo della discesa del gradiente con momentum è stato studiato per accelare la convergenza e aiutare l'algoritmo a ridurre le oscillazioni. \n",
    "w(k+1) = wk + l.r.*vk, dove vk = velocità accumulata al passo precedente, la quale serve pper aumentare le dimensioni degli aggiornamenti quando i gradienti puntano nella stessa direzione, mentre le riduce se i gradienti puntano in senso opposto. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e4712",
   "metadata": {},
   "source": [
    "3. Learning rate scheduling: step decay, decadimento esponenziale, decadimento dipendente dal tempo. **Punti 1**\n",
    " - Learning rate adattivo: Adagrad, RMSProp, Adadelta, Adam. **Punti 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817419e7",
   "metadata": {},
   "source": [
    "Step decay: riduce il learning rate per ogni epoca di un fattore delta\n",
    "Decadimento esponenziale: varia in base alle iterazioni, seguendo la regola: lr = lr * e**(-kt)\n",
    "Decadimento basato sul tempo: divide il learning rate iniziale in base al numero di iterazioni eseguite. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d79f81",
   "metadata": {},
   "source": [
    "Se il learning rate viene scelto troppo elevato, allora si rischia di non convergere mai alla soluzione del problema. D'altra parte, qualora si scelga un learning rate troppo basso, si giunge alla soluzione del problema in un numero di iterazioni troppo elevate.\n",
    "Dunque, la scelta migliore consiste nell'avere un learning rate dinamico che nelle fasi di ricerca è medio alto, mentre nelle fasi finali è basso.\n",
    "\n",
    "Adagrad = adatta il learning rate ai parametri, eseguendo aggiornamenti grandi ai parametri poco frequenti, e viceversa.In questo modo, il learning rate si adatta alle caratteristiche dei dati e del modello. Tuttavia, si ha l'accumulo di gradienti quadrati, che può portare a un learning rate molto basso. \n",
    "\n",
    "RMSProp = deriva da adagrad, con la differenza che migliora la parte di accumulto del gradiente con una media ponderata esponenziale dei gradienti al quadrato. \n",
    "\n",
    "Adadelta = come RMSProp, mma non richiede di impostare un learning rate iniziale, dato che utilizza la quantità di cambiamento stessa come calibrazione per i cambiamenti successivi. \n",
    "\n",
    "Adam = utilizza RMSProp e il metodo del gradiente con momentum. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
